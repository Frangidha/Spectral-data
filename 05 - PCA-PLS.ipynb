{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# Modeling using regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "# Spectral Data Analysis with PCA and PLS\n",
    "\n",
    "This repository contains Jupyter Notebook files for performing spectral data analysis using Principal Component Analysis (PCA) and Partial Least Squares (PLS) techniques. The analysis aims to identify important wavelengths in the raw spectra and build a predictive model for further insights.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this analysis, we utilize Jupyter Notebook as an interactive computational environment to explore, preprocess, and model spectral data. The main objectives and steps involved are:\n",
    "\n",
    "1. **Data Loading**: Load the raw spectral data into the notebook.\n",
    "2. **Data Preprocessing**: Preprocess the raw spectra if needed (e.g., normalization, baseline correction, smoothing).\n",
    "3. **Principal Component Analysis (PCA)**:\n",
    "   - Apply PCA to the preprocessed spectral data.\n",
    "   - Visualize the explained variance ratio to determine the number of principal components to retain.\n",
    "   - Plot the scores and loadings plots to explore the data structure and identify important wavelengths.\n",
    "4. **Partial Least Squares (PLS)**:\n",
    "   - Split the data into training and testing sets.\n",
    "   - Apply PLS regression to build a predictive model using the training set.\n",
    "   - Evaluate the model performance on the testing set (e.g., using metrics like R-squared, RMSE).\n",
    "   - Visualize the predicted versus actual values to assess model performance.\n",
    "5. **Conclusion and Interpretation**: Summarize the findings from PCA and PLS analyses, interpret the results, and discuss potential implications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/PP5-Housing-issue/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory.\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/PP5-Housing-issue'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSpFreVRiuM3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xk7DU_ekbtX8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>...</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>706</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>548</td>\n",
       "      <td>RFn</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>978</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>460</td>\n",
       "      <td>RFn</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mn</td>\n",
       "      <td>486</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608</td>\n",
       "      <td>RFn</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  BedroomAbvGr BsmtExposure  BsmtFinSF1 BsmtFinType1  \\\n",
       "0       856     854.0           3.0           No         706          GLQ   \n",
       "1      1262       0.0           3.0           Gd         978          ALQ   \n",
       "2       920     866.0           3.0           Mn         486          GLQ   \n",
       "\n",
       "   BsmtUnfSF  EnclosedPorch  GarageArea GarageFinish  ...  LotFrontage  \\\n",
       "0        150            0.0         548          RFn  ...         65.0   \n",
       "1        284            NaN         460          RFn  ...         80.0   \n",
       "2        434            0.0         608          RFn  ...         68.0   \n",
       "\n",
       "   MasVnrArea OpenPorchSF  OverallCond  OverallQual  TotalBsmtSF  WoodDeckSF  \\\n",
       "0       196.0          61            5            7          856         0.0   \n",
       "1         0.0           0            8            6         1262         NaN   \n",
       "2       162.0          42            5            7          920         NaN   \n",
       "\n",
       "   YearBuilt  YearRemodAdd  SalePrice  \n",
       "0       2003          2003     208500  \n",
       "1       1976          1976     181500  \n",
       "2       2001          2002     223500  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = (pd.read_csv(\"outputs/datasets/collection/house_prices_records.csv\")\n",
    "  )\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ofil7xTpm6l9"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krjAk78Tbyhv"
   },
   "source": [
    "# Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfCsXhBYVBJw"
   },
   "source": [
    "## ML pipeline for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C6keis6ao8LA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codeany/.local/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def snv_dataframe(df):\n",
    "    \"\"\"\n",
    "    Perform Standard Normal Variate (SNV) transformation on spectra in a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing spectral data. Each row represents a sample,\n",
    "                            and each column represents a wavelength.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing SNV-transformed spectra.\n",
    "    \"\"\"\n",
    "    # Calculate mean and standard deviation of each spectrum\n",
    "    mean_spectrum = df.mean(axis=1)\n",
    "    std_spectrum = df.std(axis=1)\n",
    "    \n",
    "    # Perform SNV transformation\n",
    "    snv_df = (df.sub(mean_spectrum, axis=0)).div(std_spectrum, axis=0)\n",
    "    \n",
    "    return snv_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'raw_data.csv' contains your raw spectral data in a CSV file with each row representing a sample,\n",
    "# and each column representing a wavelength.\n",
    "\n",
    "# Load raw spectral data into a pandas DataFrame\n",
    "raw_df = pd.read_csv('raw_data.csv')\n",
    "\n",
    "# Perform SNV transformation on the DataFrame\n",
    "snv_df = snv_dataframe(raw_df)\n",
    "\n",
    "# Save SNV-transformed data to a CSV file\n",
    "snv_df.to_csv('snv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_7BXNYMULrf"
   },
   "source": [
    "## PCA optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kMgswohfKBda"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_pca_variance(pca):\n",
    "    \"\"\"\n",
    "    Plot the explained variance ratio of PCA components.\n",
    "    \n",
    "    Parameters:\n",
    "    pca (sklearn.decomposition.PCA): PCA object fitted to data.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='-')\n",
    "    plt.title('Explained Variance Ratio')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_cumulative_variance(pca):\n",
    "    \"\"\"\n",
    "    Plot the cumulative explained variance ratio of PCA components.\n",
    "    \n",
    "    Parameters:\n",
    "    pca (sklearn.decomposition.PCA): PCA object fitted to data.\n",
    "    \"\"\"\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='-')\n",
    "    plt.title('Cumulative Explained Variance Ratio')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'snv_data.csv' contains your SNV-transformed spectral data in a CSV file.\n",
    "\n",
    "# Load SNV-transformed spectral data into a pandas DataFrame\n",
    "snv_df = pd.read_csv('snv_data.csv')\n",
    "\n",
    "# Perform PCA on the DataFrame\n",
    "pca = PCA()\n",
    "pca.fit(snv_df)\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plot_pca_variance(pca)\n",
    "\n",
    "# Plot cumulative explained variance ratio\n",
    "plot_cumulative_variance(pca)\n",
    "\n",
    "# Continue with further analysis as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXu0Ryeown7N"
   },
   "source": [
    "## PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "four type of models show a very strong correlation max-score. ExtraTreesRegressor, Linear Regression, GradientBoostingRegressor & RandForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewezVDt46jTJ"
   },
   "source": [
    "### Do an extensive search on the most suitable algorithm to find the best hyperparameter configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1WozH5frBQ9"
   },
   "source": [
    "Define model and parameters, for Extensive Search\n",
    "\n",
    "where obtained from code institue curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sDT_WMUErBRB"
   },
   "outputs": [],
   "source": [
    "\n",
    "models_search = {\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesRegressor\":{'model__n_estimators': [100,50,150],\n",
    "                           'model__max_depth': [None, 3, 15],\n",
    "                           'model__min_samples_split': [2, 50],\n",
    "                           'model__min_samples_leaf': [1,50],\n",
    "    },\n",
    "\n",
    "    \"LinearRegression\":{},\n",
    "\n",
    "    \"RandomForestRegressor\":{'model__n_estimators': [100,50, 140],\n",
    "                             'model__max_depth': [None,4, 15],\n",
    "                             'model__min_samples_split': [2,50],\n",
    "                             'model__min_samples_leaf': [1,50],\n",
    "                             'model__max_leaf_nodes': [None,50],\n",
    "    },\n",
    "    \n",
    "    \"GradientBoostingRegressor\":{'model__n_estimators': [100,50,140],\n",
    "                                 'model__learning_rate':[0.1, 0.01, 0.001],\n",
    "                                 'model__max_depth': [3,15, None],\n",
    "                                 'model__min_samples_split': [2,50],\n",
    "                                 'model__min_samples_leaf': [1,50],\n",
    "                                 'model__max_leaf_nodes': [None,50],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'data.csv' contains your spectral data and 'targets.csv' contains corresponding class labels for classification.\n",
    "\n",
    "# Load spectral data and targets into pandas DataFrames\n",
    "data_df = pd.read_csv('data.csv')\n",
    "targets_df = pd.read_csv('targets.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_df, targets_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PLS Discriminant Analysis (PLS-DA)\n",
    "pls_da = PLSDA(n_components=2)\n",
    "pls_da.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict classes on the testing set\n",
    "y_pred_classes = pls_da.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Accuracy Score (PLS-DA):\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Modeling and Evaluation - Predict Customer Churn.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
